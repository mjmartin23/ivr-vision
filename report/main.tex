% You can write any comments you want as long as there is a percentage sign at the beginning. This won't appear in your document.

\documentclass[12pt]{article}
%	options include 12pt or 11pt or 10pt
%	classes include article, report, book, letter, thesis, with 'article' being the default layout

\usepackage{amssymb,amsmath,textcomp} %These are optional packages you can install which gives you more mathematical symbols to play with

\usepackage{graphicx,subfig,enumerate,rotating} 
\graphicspath{{img/}}


%%%%%%%%%%%%%%%%%
% CUSTOM COMMANDS



%%%%%%%%%%%%%%%%%


\title{Introduction to Vision and Robotics\\Vision Practical: Coin Counter}
\author{Dylan Angus, Matthew Martin}
\date{\today}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% OUTLINE
% Introduction
%	a paragraph outlining the task and main ideas we used to solve it
% Methods
%	for each technique we used for background subtraction
%		list their advantages and disadvantages
%		provide images of their output
%		explain why we decided to/not to use it
%	for each technique of object segmentation/separation
%		list their advantages and disadvantages
%		provide images of their output
%		explain why we decided to/not to use it
%	explain our classifier
% Results
%	provide data (e.g. confusion matrix) for how our classifier performed under testing
%	show output images from each stage of our program
% Discussion
%	discuss the success of our program, how it could be improved, challenges/limitations we faced
% Code
%	attach all matlab code that we wrote (not anything downloaded from course website)as an appendix

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
	
\maketitle

\section{Introduction}

The purpose of this practical is to develop a program in Matlab that recognises and classifies several objects in an image. These objects can be coins or other small items, and the program must segment the image, identify each of the objects, and output the total value (in pounds and pence) of the objects in the image.

All of the images are taken from a downward facing camera viewing a scene containing the objects on a static background. We were provided with a set of 14 sample images on which to train our classifier (see Figure \ref{fig:samplescene} for an example).

\begin{figure}
	\centering
	\includegraphics[width=0.75\linewidth]{02}
	\label{fig:samplescene}
	\caption{This is one of the test images given to train the classifier.}
\end{figure}

The following are the objects and associated values that may or may not be present in any given image:
\begin{itemize}
	\item one and two pound pieces
	\item 50, 20, and 5 pence pieces
	\item washer with small hole (75p)
	\item washer with large hole (25p)
	\item angle bracket (2p)
	\item AAA battery (no value)
	\item nut (no value)
\end{itemize}

We approached this problem by dividing it into three distinct stages: background segmentation, object detection, and object classification.

\section{Methods}

\subsection{Background Segmentation}

Creating a reliable algorithm that would clearly segment the background from the objects of interest in the image was the most time-consuming and challenging section of this project. We tried several different methods of background segmentation, to varying degrees of success. We ended up choosing median filter thresholding as our most successful method.

\subsubsection{Naive thresholding}

Our algorithm for creating a naive threshold can be described by the following steps:
\begin{enumerate}
	\item Attain the median values for each of the three color channels, $r,g,b$ in the given image
	\item For each pixel, if that pixel's color values are $\pm20$ from the median, label it as a background pixel. Else, label it as an object of interest.
\end{enumerate}

This method has a few advantages. It is fast, as it only requires two passes over the entire image, and there are no computationally expensive operations inside of the loop. It is simple and easy to understand. However, this method fails to accommodate for shadows in the background. It also needs to be tuned specifically to the image (the range of $\pm20$ from the medians was chosen by trial and error). Even after careful tuning, this algorithm still miss-classifies some pixels. See Figure \ref{fig:naive} for an example of the output of this method.

\subsubsection{Adaptive thresholding}

Adaptive thresholding, as opposed to naive thresholding, generates a unique threshold value for a set of sub-images inside the given image. This is meant to allow for shadows to fall on the background and still be classified as background since, the threshold is a more localized value.

This was a fairly successful method, but still had its share of disadvantages. Adaptive thresholding highlighted the edges around some of the objects rather than the objects themselves, but for others, identified the body of the object correctly. This inconsistency would cause problems when trying to classify the object. However, we never had any problems with background shadows when using this method. See the results in Figure \ref{fig:adapt}.

\subsubsection{RGB normalization}

This algorithm is meant to reduce background shadows as well by normalizing the intensity of a pixel color. It adjusts the $r,g,b$ values based on the following division:
\[r=\frac{r}{r+g+b},\hspace{5mm}g=\frac{g}{r+g+b},\hspace{5mm}b=\frac{b}{r+g+b}.\]
Then, a tight threshold can be created based on the now very similar background values. 

RGB normalization has a lot of advantages. Like adaptive thresholding, it is effective at ignoring shadows in the background. It also runs very quickly since the normalization takes advantage of Matlab's efficiency in vectorized operations, and then requires only one pass over the pixels to threshold. It produces fairly consistently good results, rarely classifying background pixels as an object. However, it often misses a couple objects in each image, which is unpredictable and would be problematic for trying to get an accurate money total for all objects in the scene. See Figure \ref{fig:norm_rgb} for an image segmented by RGB normalization.

\subsubsection{K-means clustering}

We also tried using a K-means clustering algorithm to segment the background from the foreground. This algorithm is meant to cluster all of the background pixels into the same class and then cluster all of the objects into the same class.

We saw limited success using this strategy, as it often fails to classify shadows in the background as part of the background. It is also fairly slow, as it has to run for several iterations before the clusters converge. See the results from this method in Figure \ref{fig:kmeans}.

\subsubsection{Mean-shift segmentation}

Mean-shift segmentation is a commonly recommended method for background subtraction, because, like adaptive thresholding, it localizes its segmentation to subsets of the pixels in the image. The algorithm searches for local maxima in the data and identifies that as an approximate for the background.

The performance of this algorithm varies tremendously throughout different testing images. In some images, it separates out the background almost perfectly, while in other images (see Figure \ref{fig:mean_shift}) it localizes the maximum as a background shadow, and classifies the shadow as the background. It is also a time consuming algorithm. Ultimately, we could not reduce the inconsistency in performance.

\subsubsection{Median filter thresholding}

Median filter thresholding involves using the entire dataset of images for background segmentation. This algorithm calculates the median $r,g,b$ values for each pixel throughout all of the images in the set. Then we are able to compute a pixel-by-pixel threshold for the image being segmented. We knew that the median $r,g,b$ values would correspond to the background color for that pixel location, so we put a threshold on the sum of the absolute values of the differences between the median $r,g,b$ value at a pixel and the actual $r,g,b$ value at that pixel. Here is the threshold represented mathematically, for a single pixel and a threshold $T$:
\[abs(r_{median}-r)+abs(g_{median}-g)+abs(b_{median}-b)<T\]

This algorithm consistently performed very well. It ran fairly quickly, especially after taking advantage of Matlab's vector operation optimizations. It always classified background shadows as part of the background, and usually found most of each object. We decided to use this method for our background segmentation algorithm. See Figure \ref{fig:median} for a sample output image.

\begin{figure}
	
	\centering
	\subfloat[naive thresholding] {
		\includegraphics[width=0.5\linewidth]{naive}
		\label{fig:naive}
	}
	\subfloat[adaptive thresholding] {
		\includegraphics[width=0.5\linewidth]{adapt}
		\label{fig:adapt}
	}
	\hspace{0mm}
	\subfloat[RGB normalization] {
		\includegraphics[width=0.5\linewidth]{norm_rgb}
		\label{fig:norm_rgb}
	}
	\subfloat[k-means classification] {
		\includegraphics[width=0.5\linewidth]{kmeans}
		\label{fig:kmeans}
	}
	\hspace{0mm}
	\subfloat[mean-shift segmentation] {
		\includegraphics[width=0.5\linewidth]{mean_shift2}
		\label{fig:mean_shift}
	}
	\subfloat[median filter thresholding] {
		\includegraphics[width=0.5\linewidth]{median1}
		\label{fig:median}
	}
	
	\label{fig:bgseg}
	\caption{Here are the output images for the six methods of background segmentation that we tried.}
	
\end{figure}

\subsection{Object Detection}

We tried a couple different methods for detecting each object from the segmented image. The success of these methods is largely dependent on how well the background is separated from the original image. We settled on detecting boundaries in the image as the most reliable method to recognise discrete objects in the segmented image.

\subsubsection{K-means clustering}

We attempted to use k-means clustering in order to separate out the different objects in the image. The idea was to have each cluster correspond to an object in the image. However, we realized that this would not be practical to use generally because the k-means algorithm requires the number of clusters as a parameter. Since we do not know how many objects are in each image before processing it, we do not have this information for the classifier.

\subsubsection{Boundary detection}

Boundary detection works extremely well to separate out each object. The only issue is that sometimes the background segmentation breaks up an object into a few pieces. We solved this problem by doing some pre-processing on the image before passing it to the boundary detector. The pre-processing algorithm is structured as follows:
\begin{itemize}
	\item For each pixel in the image:
	\item If the pixel is white (part of an object), and the pixel that is 10 rows below it is also white, then connect the two pixels
\end{itemize}
This algorithm effectively joins up objects that are meant to be together, thus making it so they share a boundary. This image is then passed into a boundary detector, and then we return a logical matrix that is 3-dimensional: rows, columns, and a layer for each object in the image. See Figure \ref{fig:bound} for the output of this process.

\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{boundary}
	\caption{Here is an image after background subtraction and boundary detection. Each color corresponds to a unique object as identified by its boundary.}
	\label{fig:bound}
\end{figure}

\subsection{Classification}

The classification process consisted of two main parts: feature selection and classifier training.

\subsubsection{Feature selection}

Feature selection is an extremely important stage in classification. Choosing features that allow the classifier to make clear distinctions between different objects in the image is essential to the success of the classifier. We chose to extract the following features from each object in the image for our classifier:

\begin{itemize}
	\item Mean $r,g,b$ values
	\item 1 complex moment (c1)
	\item SURF features
	\item Compactness
\end{itemize}

These features were calculated given a subimage of the overall image that contains only the object of interest. The mean RGB values were stored along side its compactness and c1 moments  of the subimage in a feature vector.  See Figure \ref{fig:5pencesub} for an example of a 5 pence piece that would be analyzed for these features.

The process of deciding to use this feature set relied on trial and error. We attempted several other combinations as well, but achieved worse results as compared to the accuracy of these features. We tried to use simply mean $r,g,b$ values and 6 complex moments. In this instance, the extra several complex moments did not seem to improve performance, so we eliminated them. We tried adding in major axis length and minor axis length, however, this was only useful in separating out the batteries and angle brackets, which were already being classified fairly accurately.

\begin{figure}
	\centering
	\includegraphics[width=0.5\linewidth]{subimage}
	\caption{A five pence piece extracted from a training image.}
	\label{fig:5pencesub}
\end{figure}

\subsubsection{Classifier training}
 
Upon testing the multivariate gaussian classifier with all the features in the feature vector grouped together, we realized that there wasn't enough training data to properly approximate the covariance matrix. Thus, we then chose to approximate the distribution by using a Naive Bayes classifier instead. By assuming independence between the group of RGB means and group containing the compactness and c1 moment together, we used a multivariate gaussian distribution to approximate each of their conditional distributions and then found the class which maximised the product of their conditional probabilities and prior probabilities of the classes. Doing this the performance rose drastically.

\section{Results}

In order to assess the accuracy of our classification model, we split the data into a training and a testing set, giving 75\% to training and 25\% to testing. We calculated $precision$ and $recall$ values for each class. These two measures are widely used to assess the performance of classification models and are calculated as follows (where $tp=TruePositives,fp=FalsePositives,fn=FalseNegatives$):

\[precision=\frac{tp}{tp+fp},\hspace{5mm}recall=\frac{tp}{tp+fn}\]

We calculated average precision and recall across each of the ten classes in the testing data and achieved the following results: 

\[precision=0.61,\]
\[recall=0.65.\]

We generated a confusion matrix from our testing data. This can be seen in Table \ref{tab:conf}. Figure \ref{fig:pipeline} shows the output of our program at each stage of the process, as described in the Methods section.

\begin{sidewaystable}[]
	\centering
	\caption{Confusion matrix for data from testing classification. Zeros were omitted for readability.}
	\label{tab:conf}
	\begin{tabular}{l|l|l|l|l|l|l|l|l|l|l}
					& AAA & abracket & 50p & 5p & nut & 1pound & 20p & 2pound & washlghole & washsmhole \\ \hline
		AAA 		& 3   &          &     & 	&     & 	   &     &        &            &            \\ \hline
		abracket 	&     & 3        &     &  	&     & 	   &     &        &            &            \\ \hline
		50p 		&     &          & 1   & 	&     & 	   &     & 1      &            &            \\ \hline
		5p 			&     & 1        &     & 2	&     & 	   &     &        &            &            \\ \hline
		nut 		&     & 2    	 &     & 	&     & 	   &     &        &            & 2          \\ \hline
		1pound 		&     & 		 &     & 	&     & 	   & 1   & 2      &            & 1          \\ \hline
		20p		 	&     & 		 &     & 	&     & 	   & 2   & 1      &            &            \\ \hline
		2pound 		&     & 		 &     & 	&     & 	   &     & 4      &            &            \\ \hline
		washlghole 	&     & 		 &     & 	&     & 	   & 1   &        & 8          &            \\ \hline
		washsmhole 	&     & 		 &     & 	& 1   & 	   &     &        &            & 4         
	\end{tabular}
\end{sidewaystable}

\begin{figure}
	\centering
	\subfloat[original image] {
		\includegraphics[width=0.32\linewidth]{02}
	}
	\subfloat[background segmented] {
		\includegraphics[width=0.38\linewidth]{median02}
	}
	\subfloat[object extraction] {
		\includegraphics[width=0.3\linewidth]{1pound}
	}
	\hfill
	\subfloat[classification] {
		\includegraphics[width=\linewidth]{classified02}
	}
	\caption{This shows the overall pipeline of our program. The original image is segmented, objects are extracted, and then classified.}
	\label{fig:pipeline}
\end{figure}


\section{Discussion}

Overall, we are satisfied with how our classifier performs. That said, even though it has many strengths, it is also hindered by a few limitations.

\subsection{Strengths}

fast

pretty accurate

invariant to background color

\subsection{Limitations}

small training set

no "unclassified" class - would probably be safer

class imbalance in testing data (very few nuts and 1pound coins)

\section*{Appendix}

\begin{verbatim}
	
	code can go here
	
\end{verbatim}


\end{document}